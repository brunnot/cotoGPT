# Projeto
Uso o Ollama como gerênciados / executor dos modelos.
Fiz teste com os modelos
* **llama2**:   `ollama pull llama2`
* **mistral**:  `ollama pull mistral` 

Para o desenvolvimento do assistente, estou usando o langchain

### Dependências
* langchain
* streamlit
* streamlit-chat
* pypdf
* chromadb
* fastembed

`pipenv install langchain streamlit streamlit_chat chromadb pypdf fastembed`

### Referências
https://python.langchain.com/docs/get_started/introduction
https://ollama.com/library
